{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17b3b4d-54ea-47d7-ab14-42e7b42914de",
   "metadata": {},
   "source": [
    "# Finetune an OSS model for out bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed8576-edd5-400b-b563-b3c16b4e3617",
   "metadata": {},
   "source": [
    "We will use the [trl]() library to make our life easy! Most of the code comes from the official [trl finetune example](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0fd751-8abf-4313-9158-26bb68751160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install accelerate transformers datasets bitsandbytes peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9961a849-6c5e-4d3f-8f87-d46406a492ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "\n",
    "import wandb\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from ft_utils import load_from_artifact, LLMSampleCB, generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc019a42-86d6-4f33-bec7-af7f73c238d9",
   "metadata": {},
   "source": [
    "What is really handy here is the data preprocessing that is baked into the `SFTTrainer` class, this trainer is a thing wrapper around the transformer's `Trainer` but adds the necessary preprocessing needed to format and pack our instruction dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ab8be-1460-4aaf-891a-99f1d92d5a41",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd96c4-320e-463d-a664-d3b512598e72",
   "metadata": {},
   "source": [
    "We will grab our dataset previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0f25b6-c42a-43a5-b50e-d6f983ac6d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data_path = \"dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6a21fd-2678-4a64-93a2-504b23d7da8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# by default the split is called train\n",
    "ds = load_dataset(\"json\", data_files=f\"{training_data_path}/*.json\")[\"train\"].shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c77dcc18-c244-421a-a3e7-5e5974ffef5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user', 'answer'],\n",
       "    num_rows: 616\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2563f6ea-f76b-4b72-b9e1-e409e8994e07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': ['and if you were asked, I would have known all your feelings of life.',\n",
       "  'Let me see that.',\n",
       "  'Okay. Okay. Okay. Okay. Okay.'],\n",
       " 'answer': ['other()', 'other()', 'other()']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7741ef5b-f1c8-475e-95b8-7cd2676ad513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splitted_ds = ds.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a32cb-32af-4b76-a2fb-ddcba8603178",
   "metadata": {},
   "source": [
    "Let's save this split in Hugging Face dataset format (fast parquet files unde the hood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd0eb455-145b-4e99-bb63-3fc07de87216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d4f966d8fe417d96b5499ece434abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/554 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f30a8e9aed422787bc9eaefe7ff840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/62 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splitted_ds.save_to_disk(f\"{training_data_path}/split_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b74605-e102-4408-a450-9cc2d373531d",
   "metadata": {},
   "source": [
    "Let's save this to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5378d73a-4b2b-4e54-b371-7bc50cc218b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with wandb.init(project=\"otto\", job_type=\"data_split\"):\n",
    "#     at = wandb.Artifact(name=\"split_dataset\",\n",
    "#                         type=\"dataset\",\n",
    "#                         description=\"The generated data splitted in 90/10\")\n",
    "#     at.add_dir(f\"{training_data_path}/split_dataset\")\n",
    "#     wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59f0e351-95fe-45e1-b53c-d1ff4fb3aabc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_ARTIFACT = 'capecape/otto/split_dataset:v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecdff480-ab3d-40a1-9900-5ebc84c6ba76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['user', 'answer'],\n",
       "        num_rows: 554\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['user', 'answer'],\n",
       "        num_rows: 62\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_from_artifact(DATASET_ARTIFACT)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e7461-3296-43ba-b6ef-35b1ec480ac0",
   "metadata": {},
   "source": [
    "## Prepare data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a232f-a2f6-4f0c-9269-f128b3c2fba6",
   "metadata": {},
   "source": [
    "> Depending on the model you will need to change this formatting function\n",
    "\n",
    "We will train a Llama2 model from MetaAI, depending if it is the `chat` or `vanilla` version, you will need to format your instructions differently. My to go place to find these format is the hugginface model card (but many times it is missing), the official paper (can be hard to find) or the [Axolotl training library](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/src/axolotl/prompt_strategies/llama2_chat.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8528f94-122b-4dc7-bbf0-4b1dc11c23d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are AI that converts human request into api calls. \n",
    "You have a set of functions:\n",
    "-news(topic=\"[topic]\") asks for latest headlines about a topic.\n",
    "-math(question=\"[question]\") asks a math question in python format.\n",
    "-notes(action=\"add|list\", note=\"[note]\") lets a user take simple notes.\n",
    "-openai(prompt=\"[prompt]\") asks openai a question.\n",
    "-runapp(program=\"[program]\") runs a program locally.\n",
    "-story(description=[description]) lets a user ask for a story.\n",
    "-timecheck(location=\"[location]\") ask for the time at a location. If no location is given it's assumed to be the current location.\n",
    "-timer(duration=\"[duration]\") sets a timer for duration written out as a string.\n",
    "-weather(location=\"[location]\") ask for the weather at a location. If there's no location string the location is assumed to be where the user is.\n",
    "-other() should be used when none of the other commands apply\n",
    "\n",
    "Here is a user request, reply with the corresponding function call, be brief.\n",
    "USER_QUERY: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e9b126-ad58-4f45-8afd-4a071ef75826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _create_mistral_instruct_prompt(user, answer=\"\"):\n",
    "    return (\"[INST] {system_prompt}{user} [/INST]\"\n",
    "            \"{answer}\").format(user=user, answer=answer, system_prompt=system_prompt)\n",
    "\n",
    "def create_mistral_prompt(row): return _create_mistral_instruct_prompt(**row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5fa779d-31ae-4574-9554-166d3821cc15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _create_llama_prompt(user, answer=\"\"):\n",
    "    \"Format the prompt to style\"\n",
    "    return (\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\"\n",
    "            \"### User: {user}\\n\"\n",
    "            \"### Answer: {answer}\").format(user=user, answer=answer)\n",
    "\n",
    "def create_prompt(row): return _create_llama_prompt(**row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "719288d3-418b-49f1-8028-a541eeebd19c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "### User: I'll get this flowy.\n",
      "### Answer: other()\n"
     ]
    }
   ],
   "source": [
    "print(create_prompt(ds[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89db80a1-94f0-4935-a5ea-e9d04a31d87d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] You are AI that converts human request into api calls. \n",
      "You have a set of functions:\n",
      "-news(topic=\"[topic]\") asks for latest headlines about a topic.\n",
      "-math(question=\"[question]\") asks a math question in python format.\n",
      "-notes(action=\"add|list\", note=\"[note]\") lets a user take simple notes.\n",
      "-openai(prompt=\"[prompt]\") asks openai a question.\n",
      "-runapp(program=\"[program]\") runs a program locally.\n",
      "-story(description=[description]) lets a user ask for a story.\n",
      "-timecheck(location=\"[location]\") ask for the time at a location. If no location is given it's assumed to be the current location.\n",
      "-timer(duration=\"[duration]\") sets a timer for duration written out as a string.\n",
      "-weather(location=\"[location]\") ask for the weather at a location. If there's no location string the location is assumed to be where the user is.\n",
      "-other() should be used when none of the other commands apply\n",
      "\n",
      "Here is a user request, reply with the corresponding function call, be brief.\n",
      "USER_QUERY: I'll get this flowy. [/INST]other()\n"
     ]
    }
   ],
   "source": [
    "print(create_mistral_prompt(ds[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38a83c10-f88a-4085-a124-3a8c7c5c99d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# MODEL_NAME = 'meta-llama/Llama-2-7b-hf'\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Define and parse arguments.\n",
    "script_args=SimpleNamespace(\n",
    "    model_name=MODEL_NAME,               # \"the model name\"\n",
    "    dataset_artifact=DATASET_ARTIFACT,   # \"the W&B artifact holding the dataset\n",
    "    log_with=\"wandb\",                    # \"use 'wandb' to log with wandb\"\n",
    "    learning_rate=1.4e-5,                # \"the learning rate\"\n",
    "    batch_size=2,                        # \"the batch size\", 24GB -> 2, 40GB -> 4\n",
    "    seq_length=400,                      # \"Input sequence length\"\n",
    "    gradient_accumulation_steps=16,      # \"simulate larger batch sizes\"\n",
    "    load_in_x_bits=4,                    # \"load the model in 4/8 precision\n",
    "    use_peft=True,                       # \"Wether to use PEFT or not to train adapters\"\n",
    "    output_dir=\"output\",                 # \"the output directory\"\n",
    "    peft_lora_r=64,                      # \"the rank of the matrix parameter of the LoRA adapters\"\n",
    "    peft_lora_alpha=16,                  # \"the alpha parameter of the LoRA adapters\"\n",
    "    logging_steps=1,                     # \"How often to log\"\n",
    "    use_auth_token=True,                 # \"Use HF auth token to access the model\"\n",
    "    max_steps=500,                       # \"the number of training steps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015974c7-a1ff-4687-ac33-0a626067f96e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7612a-87f1-4e94-9e8d-9cf0979613e2",
   "metadata": {},
   "source": [
    "We can load the model with all the bells and whistles from Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62b6c7e8-e5ee-4cdf-9534-87ca90a6e5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd7a2d8b2c542d9a55a93ec43e663f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Load the model\n",
    "if script_args.load_in_x_bits in [4,8]:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=script_args.load_in_x_bits==8, \n",
    "        load_in_4bit=script_args.load_in_x_bits==4\n",
    "    )\n",
    "else:\n",
    "    quantization_config = None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_auth_token=script_args.use_auth_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d507a281-98c4-4454-8ce0-ed54066deddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=script_args.output_dir,\n",
    "    per_device_train_batch_size=script_args.batch_size,\n",
    "    per_device_eval_batch_size=script_args.batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    logging_steps=script_args.logging_steps,\n",
    "    # num_train_epochs=script_args.num_train_epochs,\n",
    "    max_steps=script_args.max_steps,\n",
    "    report_to=script_args.log_with,\n",
    ")\n",
    "\n",
    "\n",
    "# Step 4: Define the LoraConfig\n",
    "if script_args.use_peft:\n",
    "    peft_config = LoraConfig(\n",
    "        r=script_args.peft_lora_r,\n",
    "        lora_alpha=script_args.peft_lora_alpha,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "else:\n",
    "    peft_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfec5c-3c8b-42f2-8cc7-28f33c13d39d",
   "metadata": {},
   "source": [
    "Now we need to instantiate the `SFTTrainer` with the correct preprocessing:\n",
    "- We want to pack sequences to a certain length (longer means more memory usage)\n",
    "- We want to tokenize\n",
    "- We also want to apply our prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67c08d2c-0830-4510-b32c-ed99e3844b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_args.seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e91cec6-c81d-4331-855c-23f873487268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args.eval_steps = training_args.max_steps // 5\n",
    "training_args.evaluation_strategy = \"steps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e46da0fc-8af8-4730-b218-6db315b75710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# wandb.init(project=\"otto\", job_type=\"finetune\")\n",
    "    \n",
    "ds = load_from_artifact(DATASET_ARTIFACT)\n",
    "    \n",
    "# Step 5: Define the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    args=training_args,\n",
    "    max_seq_length=script_args.seq_length,\n",
    "    packing=True,\n",
    "    formatting_func=create_mistral_prompt,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd2e28-2059-4bf5-bdfa-d162d6a35978",
   "metadata": {},
   "source": [
    "to be sure, let's check the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab1daacc-5a73-40a3-9ec8-cfd7c0f852b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'user is.\\n-other() should be used when none of the other commands apply\\n\\nHere is a user request, reply with the corresponding function call, be brief.\\nUSER_QUERY: emotion means in the context of... [/INST]other()</s><s> [INST] You are AI that converts human request into api calls. \\nYou have a set of functions:\\n-news(topic=\"[topic]\") asks for latest headlines about a topic.\\n-math(question=\"[question]\") asks a math question in python format.\\n-notes(action=\"add|list\", note=\"[note]\") lets a user take simple notes.\\n-openai(prompt=\"[prompt]\") asks openai a question.\\n-runapp(program=\"[program]\") runs a program locally.\\n-story(description=[description]) lets a user ask for a story.\\n-timecheck(location=\"[location]\") ask for the time at a location. If no location is given it\\'s assumed to be the current location.\\n-timer(duration=\"[duration]\") sets a timer for duration written out as a string.\\n-weather(location=\"[location]\") ask for the weather at a location. If there\\'s no location string the location is assumed to be where the user is.\\n-other() should be used when none of the other commands apply\\n\\nHere is a user request, reply with the corresponding function call, be brief.\\nUSER_QUERY: Turn the brake! [/INST]other()</s><s> [INST] You are AI that converts human request into api calls. \\nYou have a set of functions:\\n-news(topic=\"[topic]\") asks for latest headlines about a topic.\\n-math(question=\"[question]\") asks a math question in python format.\\n-notes(action'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = trainer.get_train_dataloader()\n",
    "b = next(iter(dl))\n",
    "trainer.tokenizer.decode(b[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75110232-61f2-4bc5-88d1-cdbc549ad127",
   "metadata": {},
   "source": [
    "Let's sample from the model during Training, to do this we will add a custom WandbCallback that has access to the Trainer object (and model and tokenizer). Normally, callback don't have access to these, and that's why we need to add it to the instantiated Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "674b1b0a-e2c9-4314-853a-9f23d0a25767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_test_prompt = lambda row: {\"text\": _create_mistral_instruct_prompt(row[\"user\"], \"\")}\n",
    "\n",
    "test_dataset = ds[\"test\"].map(create_test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "847757b8-018a-48c3-b959-e087933e7769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] You are AI that converts human request into api calls. \\nYou have a set of functions:\\n-news(topic=\"[topic]\") asks for latest headlines about a topic.\\n-math(question=\"[question]\") asks a math question in python format.\\n-notes(action=\"add|list\", note=\"[note]\") lets a user take simple notes.\\n-openai(prompt=\"[prompt]\") asks openai a question.\\n-runapp(program=\"[program]\") runs a program locally.\\n-story(description=[description]) lets a user ask for a story.\\n-timecheck(location=\"[location]\") ask for the time at a location. If no location is given it\\'s assumed to be the current location.\\n-timer(duration=\"[duration]\") sets a timer for duration written out as a string.\\n-weather(location=\"[location]\") ask for the weather at a location. If there\\'s no location string the location is assumed to be where the user is.\\n-other() should be used when none of the other commands apply\\n\\nHere is a user request, reply with the corresponding function call, be brief.\\nUSER_QUERY: Cheers! [/INST]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = test_dataset[0][\"text\"]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "feebc470-9492-470e-83e1-8c1a3094f130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "gen_config = GenerationConfig.from_pretrained(script_args.model_name, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb27c6c5-cd10-4f54-9fde-9713a6ada9aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'other()'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(prompt, trainer.model, trainer.tokenizer, gen_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a149e-fa5d-4da7-8948-166eab833f14",
   "metadata": {},
   "source": [
    "this a already pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13672553-1a7b-4408-a661-baa5b44092b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Let's finetune to force it reply with the function call only!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d41ab-cce3-4670-9f6e-2bf8623ac7cd",
   "metadata": {},
   "source": [
    "we add the LLMSampleCB to log examples during training. Let's pick the examples first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "022f0540-bf0f-4cfd-99d5-3003b1f8a791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "hand_picked_ds = Dataset.from_list([test_dataset[0],])\n",
    "for s in test_dataset:\n",
    "    if s[\"answer\"] not in [t[\"answer\"] for t in hand_picked_ds]:\n",
    "        hand_picked_ds = hand_picked_ds.add_item(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d84675c1-2a8f-4c65-a66b-fdbf4607e120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb_cb = LLMSampleCB(trainer, test_dataset=hand_picked_ds, num_samples=8, max_new_tokens=256)\n",
    "trainer.add_callback(wandb_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4302997f-b5e6-4115-bd03-17b0658a78e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/cape/otto/finetune/wandb/run-20231028_182043-6urzaw17</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/huggingface/runs/6urzaw17' target=\"_blank\">usual-armadillo-26</a></strong> to <a href='https://wandb.ai/capecape/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/huggingface' target=\"_blank\">https://wandb.ai/capecape/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/huggingface/runs/6urzaw17' target=\"_blank\">https://wandb.ai/capecape/huggingface/runs/6urzaw17</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 5:40:33, Epoch 28/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.804600</td>\n",
       "      <td>0.789849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.243600</td>\n",
       "      <td>0.261140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>0.217698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.221200</td>\n",
       "      <td>0.204139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.200558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.4949857429563999, metrics={'train_runtime': 20474.192, 'train_samples_per_second': 0.781, 'train_steps_per_second': 0.024, 'total_flos': 2.740962459648e+17, 'train_loss': 0.4949857429563999, 'epoch': 28.88})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25b5e1ba-b9c6-44d5-8bae-9f22b8c2ea84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output/6urzaw17-mistralai_Mistral-7B-Instruct-v0.1-ft)... Done. 0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact 6urzaw17-mistralai_Mistral-7B-Instruct-v0.1-ft logged.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(trainer, output_dir):\n",
    "    \"Save the model to a folder inside {output_dir} prepending the run name\"\n",
    "    model_name = f\"{wandb.run.id}-{trainer.model.name_or_path}-ft\".replace(\"/\",\"_\")\n",
    "    model_folder = Path(output_dir) / model_name\n",
    "    model_folder.mkdir(parents=True, exist_ok=True)\n",
    "    trainer.save_model(model_folder)\n",
    "    return model_name, model_folder\n",
    "\n",
    "def create_model_artifact(model_name, model_folder):\n",
    "    \"Creates a Weights & Biases artifact for the saved model\"\n",
    "    at = wandb.Artifact(\n",
    "        name=model_name,\n",
    "        type=\"model\",\n",
    "        description=\"Finetuned model on Otto dataset\",\n",
    "        metadata={\"peft\":peft_config,\n",
    "                  \"quantization\":quantization_config},\n",
    "    )\n",
    "    at.add_dir(model_folder)\n",
    "    wandb.log_artifact(at)\n",
    "    print(f\"Artifact {model_name} logged.\")\n",
    "    \n",
    "def save_and_log(trainer, output_dir):    \n",
    "    model_name, model_folder = save_model(trainer, output_dir)\n",
    "    create_model_artifact(model_name, model_folder)\n",
    "\n",
    "save_and_log(trainer, script_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "167f123a-c353-46b6-b2ab-d1394c083e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▁▁▇▅</td></tr><tr><td>eval/samples_per_second</td><td>▁██▁▅</td></tr><tr><td>eval/steps_per_second</td><td>▁██▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▆▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.20056</td></tr><tr><td>eval/runtime</td><td>29.3728</td></tr><tr><td>eval/samples_per_second</td><td>2.111</td></tr><tr><td>eval/steps_per_second</td><td>1.055</td></tr><tr><td>train/epoch</td><td>28.88</td></tr><tr><td>train/global_step</td><td>500</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1774</td></tr><tr><td>train/total_flos</td><td>2.740962459648e+17</td></tr><tr><td>train/train_loss</td><td>0.49499</td></tr><tr><td>train/train_runtime</td><td>20474.192</td></tr><tr><td>train/train_samples_per_second</td><td>0.781</td></tr><tr><td>train/train_steps_per_second</td><td>0.024</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">usual-armadillo-26</strong> at: <a href='https://wandb.ai/capecape/huggingface/runs/6urzaw17' target=\"_blank\">https://wandb.ai/capecape/huggingface/runs/6urzaw17</a><br/> View job at <a href='https://wandb.ai/capecape/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDQ2ODg3Mg==/version_details/v3' target=\"_blank\">https://wandb.ai/capecape/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDQ2ODg3Mg==/version_details/v3</a><br/>Synced 6 W&B file(s), 5 media file(s), 14 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231028_182043-6urzaw17/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c90987-6e1e-4f58-8acb-6d3e8079c8ff",
   "metadata": {},
   "source": [
    "## Eval on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550fd2b0-6571-4be2-9434-7c58bebb628c",
   "metadata": {},
   "source": [
    "Run inference on the full test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9156b1ed-421d-4786-b3e0-627358c7534b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 1/62 [00:00<00:39,  1.55it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 2/62 [00:01<00:39,  1.53it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|▍         | 3/62 [00:01<00:38,  1.52it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|▋         | 4/62 [00:02<00:37,  1.53it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|▊         | 5/62 [00:03<00:37,  1.53it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|▉         | 6/62 [00:04<00:40,  1.38it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|█▏        | 7/62 [00:04<00:38,  1.42it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 13%|█▎        | 8/62 [00:05<00:42,  1.26it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|█▍        | 9/62 [00:06<00:39,  1.34it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|█▌        | 10/62 [00:07<00:37,  1.39it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|█▊        | 11/62 [00:07<00:35,  1.43it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|█▉        | 12/62 [00:08<00:38,  1.30it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21%|██        | 13/62 [00:09<00:35,  1.36it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 23%|██▎       | 14/62 [00:09<00:34,  1.40it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|██▍       | 15/62 [00:10<00:32,  1.44it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|██▌       | 16/62 [00:11<00:31,  1.46it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|██▋       | 17/62 [00:11<00:30,  1.47it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29%|██▉       | 18/62 [00:12<00:29,  1.49it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|███       | 19/62 [00:13<00:28,  1.49it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|███▏      | 20/62 [00:13<00:27,  1.50it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|███▍      | 21/62 [00:14<00:30,  1.36it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███▌      | 22/62 [00:15<00:28,  1.41it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 37%|███▋      | 23/62 [00:16<00:27,  1.44it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|███▊      | 24/62 [00:16<00:25,  1.47it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|████      | 25/62 [00:17<00:24,  1.48it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|████▏     | 26/62 [00:18<00:24,  1.49it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|████▎     | 27/62 [00:18<00:23,  1.50it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|████▌     | 28/62 [00:19<00:22,  1.50it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|████▋     | 29/62 [00:20<00:21,  1.51it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|████▊     | 30/62 [00:20<00:21,  1.51it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████     | 31/62 [00:21<00:22,  1.39it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|█████▏    | 32/62 [00:22<00:20,  1.44it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|█████▎    | 33/62 [00:22<00:19,  1.45it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|█████▍    | 34/62 [00:23<00:19,  1.47it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|█████▋    | 35/62 [00:24<00:18,  1.49it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|█████▊    | 36/62 [00:24<00:17,  1.50it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|█████▉    | 37/62 [00:25<00:16,  1.51it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|██████▏   | 38/62 [00:26<00:17,  1.36it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 63%|██████▎   | 39/62 [00:27<00:17,  1.30it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|██████▍   | 40/62 [00:28<00:17,  1.24it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|██████▌   | 41/62 [00:29<00:17,  1.20it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|██████▊   | 42/62 [00:29<00:15,  1.29it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|██████▉   | 43/62 [00:30<00:14,  1.34it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71%|███████   | 44/62 [00:31<00:12,  1.39it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|███████▎  | 45/62 [00:31<00:11,  1.43it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|███████▍  | 46/62 [00:32<00:13,  1.22it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|███████▌  | 47/62 [00:33<00:11,  1.29it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 77%|███████▋  | 48/62 [00:34<00:10,  1.35it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79%|███████▉  | 49/62 [00:34<00:09,  1.40it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|████████  | 50/62 [00:35<00:08,  1.43it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|████████▏ | 51/62 [00:36<00:07,  1.45it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████▍ | 52/62 [00:36<00:06,  1.47it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████▌ | 53/62 [00:37<00:06,  1.48it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 87%|████████▋ | 54/62 [00:38<00:06,  1.22it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|████████▊ | 55/62 [00:39<00:05,  1.30it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|█████████ | 56/62 [00:39<00:04,  1.35it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|█████████▏| 57/62 [00:40<00:03,  1.40it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|█████████▎| 58/62 [00:41<00:02,  1.43it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|█████████▌| 59/62 [00:41<00:02,  1.45it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|█████████▋| 60/62 [00:42<00:01,  1.47it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|█████████▊| 61/62 [00:43<00:00,  1.48it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "                                               \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/cape/otto/finetune/wandb/run-20231029_112451-28fa1088</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/otto/runs/28fa1088' target=\"_blank\">misty-terrain-39</a></strong> to <a href='https://wandb.ai/capecape/otto' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/otto' target=\"_blank\">https://wandb.ai/capecape/otto</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/otto/runs/28fa1088' target=\"_blank\">https://wandb.ai/capecape/otto/runs/28fa1088</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fce5b3a7b354ba0833aae72635689d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.176 MB of 0.176 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misty-terrain-39</strong> at: <a href='https://wandb.ai/capecape/otto/runs/28fa1088' target=\"_blank\">https://wandb.ai/capecape/otto/runs/28fa1088</a><br/> View job at <a href='https://wandb.ai/capecape/otto/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDQyMzQ5OQ==/version_details/v23' target=\"_blank\">https://wandb.ai/capecape/otto/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDQyMzQ5OQ==/version_details/v23</a><br/>Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231029_112451-28fa1088/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_config = GenerationConfig.from_pretrained(\n",
    "    script_args.model_name,\n",
    "    max_new_tokens=256)\n",
    "\n",
    "records_table = wandb.Table(columns=[\"prompt\", \"generation\"] + list(gen_config.to_dict().keys()))\n",
    "\n",
    "for example in tqdm(test_dataset, leave=False):\n",
    "    prompt = example[\"text\"]\n",
    "    generation = generate(prompt=prompt, \n",
    "                           model=trainer.model, \n",
    "                           tokenizer=trainer.tokenizer,\n",
    "                           gen_config=gen_config)\n",
    "    records_table.add_data(prompt, generation, *list(gen_config.to_dict().values()))\n",
    "\n",
    "with wandb.init(project=\"otto\", job_type=\"eval\"):    \n",
    "    wandb.log({\"eval_predictions\":records_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33af3ae-e7e5-475b-849b-9bdc300f3e18",
   "metadata": {},
   "source": [
    "## Merging Weights (restart kernel for this part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470a013-462d-4c3d-bc50-efa12c84dd4f",
   "metadata": {},
   "source": [
    "This part could be automated using Launch Model Automations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450c57ab-301c-48ec-b9a8-ec50d6409e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43fe3274-074a-4a95-9516-38a93e22f90b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_AT = 'capecape/huggingface/6urzaw17-mistralai_Mistral-7B-Instruct-v0.1-ft:v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c112861-2230-47ed-ab80-90c11bcd3f24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/cape/otto/finetune/wandb/run-20231029_112555-6lqs7q7h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/otto/runs/6lqs7q7h' target=\"_blank\">rich-oath-40</a></strong> to <a href='https://wandb.ai/capecape/otto' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/otto' target=\"_blank\">https://wandb.ai/capecape/otto</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/otto/runs/6lqs7q7h' target=\"_blank\">https://wandb.ai/capecape/otto/runs/6lqs7q7h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/capecape/otto/runs/6lqs7q7h?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f197bcced90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"otto\", job_type=\"convert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a85fcd-8422-467c-a568-03891be4a533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 6urzaw17-mistralai_Mistral-7B-Instruct-v0.1-ft:v0, 106.23MB. 9 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   9 of 9 files downloaded.  \n",
      "Done. 0:0:0.2\n"
     ]
    }
   ],
   "source": [
    "artifact = wandb.use_artifact(MODEL_AT, type=\"model\")\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f3206fb-a3d0-4c21-8bfe-bcb2fc0424e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82b4af8b4f943aca54b26e554e55774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        artifact_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef012b0c-2587-45fb-96ac-a8839a17f4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d70b50b-f649-4bd1-8fb8-becf200f5ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_dir = \"output/mistral/merged\"\n",
    "merged_model.save_pretrained(merged_dir, \n",
    "                             max_shard_size=\"2GB\",\n",
    "                             safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc4115d5-1e3e-4b94-b65c-be807665de49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's also save this to W&B\n",
    "model_name = f\"{artifact.name}-merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1adfc-06ce-4e1e-a5c4-b691e6c2d7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output/mistral/merged)... "
     ]
    }
   ],
   "source": [
    "at = wandb.Artifact(\n",
    "    name=f\"{wandb.run.id}-{model_name}\".replace(\":\", \"_\"),\n",
    "    type=\"model\",\n",
    "    description=\"Finetuned model on Otto dataset\",\n",
    ")\n",
    "at.add_dir(merged_dir)\n",
    "wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d1d84-08f2-4803-87c9-a7ebfd0f7578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
