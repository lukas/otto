{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2314fa-6bd3-4af9-8683-1c098eba5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a016d1e-84ae-4f28-9fb0-87bd46501dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{24000}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb953b97-69f8-4898-8a59-9be56faafb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-2-7b-hf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476823ad-fa8d-4470-8965-aa82215e824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289dec99-4e15-4f53-b31f-4118860e6673",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = create_bnb_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dce734-a1ad-450c-bb76-14d3504a1977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "  \"bnb_4bit_quant_type\": \"nf4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbnb_config = create_bnb_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47e43a-5f86-488c-b0e5-1399d723e1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e649ff054d54ee9af3b8d51f159c068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:648: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(base_model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd3fa7-f5b2-4946-b073-1fb40b91352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de15077-486e-4b6e-9a51-ee31e919052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cb118-ba52-4039-ab9f-3e32e39dffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4666cd1-86ae-4a1f-9de9-f65cab8254a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction', 'context', 'response')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    \n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df442ac-a7ef-4010-82f9-500c671dc41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7078c5-3a3c-41d6-b106-1e2045c64268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de83860-8fed-4b97-92a5-3157b9aa8317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b05d99-bf2e-416b-8657-58141ff0bbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/lukas/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading cached processed dataset at /home/lukas/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-499895df8962eaa7.arrow\n",
      "Loading cached processed dataset at /home/lukas/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-00cdfdb03a001d54.arrow\n",
      "Loading cached processed dataset at /home/lukas/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7e9d1ea528656142.arrow\n",
      "Loading cached shuffled indices for dataset at /home/lukas/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1eb594db6d5c3642.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "# Load the databricks dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02f057-2bad-4c57-ab0b-2ed56d1f80cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c1d07-3c74-4a7f-8cc3-a98608f4e376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 14999\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f64b6b4-5897-4d9e-b6df-ea8b6785aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all params: 3,540,389,888 || trainable params: 39,976,960 || trainable%: 1.1291682911958425\n",
      "torch.float32 302387200 0.08541070604255438\n",
      "torch.uint8 3238002688 0.9145892939574456\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ml2k2\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lukas/finetune/wandb/run-20230916_153019-eekk5ogd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/l2k2/huggingface/runs/eekk5ogd' target=\"_blank\">azure-river-2</a></strong> to <a href='https://wandb.ai/l2k2/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/l2k2/huggingface' target=\"_blank\">https://wandb.ai/l2k2/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/l2k2/huggingface/runs/eekk5ogd' target=\"_blank\">https://wandb.ai/l2k2/huggingface/runs/eekk5ogd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:45, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.544600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.643800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.277400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.299700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.559500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.038300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.01\n",
      "  total_flos               =   629604GF\n",
      "  train_loss               =     1.3982\n",
      "  train_runtime            = 0:00:52.13\n",
      "  train_samples_per_second =      1.535\n",
      "  train_steps_per_second   =      0.384\n",
      "{'train_runtime': 52.1326, 'train_samples_per_second': 1.535, 'train_steps_per_second': 0.384, 'total_flos': 676032502579200.0, 'train_loss': 1.3981655985116959, 'epoch': 0.01}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            max_steps=20,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "     \n",
    "    do_train = True\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "output_dir = \"results/llama2/final_checkpoint3\"\n",
    "train(model, tokenizer, dataset, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b774cc8-67b8-4f5f-9c83-fa473e3daa26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▅██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▃▃▃▃▃▂▁▂▃▃▃▃▄▄▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0.01</td></tr><tr><td>train/global_step</td><td>20</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.0383</td></tr><tr><td>train/total_flos</td><td>676032502579200.0</td></tr><tr><td>train/train_loss</td><td>1.39817</td></tr><tr><td>train/train_runtime</td><td>52.1326</td></tr><tr><td>train/train_samples_per_second</td><td>1.535</td></tr><tr><td>train/train_steps_per_second</td><td>0.384</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">azure-river-2</strong> at: <a href='https://wandb.ai/l2k2/huggingface/runs/eekk5ogd' target=\"_blank\">https://wandb.ai/l2k2/huggingface/runs/eekk5ogd</a><br/> View job at <a href='https://wandb.ai/l2k2/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk4MjIxNzkw/version_details/v0' target=\"_blank\">https://wandb.ai/l2k2/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk4MjIxNzkw/version_details/v0</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230916_153019-eekk5ogd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f7b18-8a3c-4f12-9f75-e600d9c23d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a348d0390c4545b8a4090bc6c27cc5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('results/llama2/final_merged_checkpoint/tokenizer_config.json',\n",
       " 'results/llama2/final_merged_checkpoint/special_tokens_map.json',\n",
       " 'results/llama2/final_merged_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\" \n",
    "\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = \"results/llama2/final_merged_checkpoint2\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "merged_model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\n",
    "# save tokenizer for easy inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c668b8a-3bf3-4d0d-8e5a-29087e4b443a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afaec9ce7df84b3a924d4fe0e9b7dbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('merged_model_unsafe/tokenizer_config.json',\n",
       " 'merged_model_unsafe/special_tokens_map.json',\n",
       " 'merged_model_unsafe/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"results/llama2/final_checkpoint3\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model_unsafe\")\n",
    "tokenizer.save_pretrained(\"merged_model_unsafe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd2358-6498-4e90-9178-b51dacb635d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Given this paragraph, how many public high schools are in Arlington, Virginia?\n",
      "\n",
      "Input:\n",
      "Arlington Public Schools operates the county's public K-12 education system of 22 elementary schools; 6 middle schools (Dorothy Hamm Middle School, Gunston Middle School, Kenmore Middle School, Swanson Middle School, Thomas Jefferson Middle School, and Williamsburg Middle School); and 3 public high schools (Wakefield High School, Washington-Liberty High School, and Yorktown High School). H-B Woodlawn and Arlington Tech are alternative public schools. Arlington County spends about half of its local revenues on education. For the FY2013 budget, 83 percent of funding was from local revenues, and 12 percent from the state. Per pupil expenditures are expected to average $18,700, well above its neighbors, Fairfax County ($13,600) and Montgomery County ($14,900).\n",
      "\n",
      "### Response:\n",
      "There are 3 public high schools in Arlington, Va.\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(dataset[2]['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7b4e3-44a4-4416-bf62-f63a32f35403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'model.layers.26.self_attn.v_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.norm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.27.mlp.up_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the merged model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmerged_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerged_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/modeling_utils.py:2022\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_file, shard \u001b[38;5;129;01min\u001b[39;00m shards\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[1;32m   2021\u001b[0m         \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[0;32m-> 2022\u001b[0m         \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2024\u001b[0m         save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/safetensors/torch.py:281\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    251\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    252\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    253\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m ):\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/safetensors/torch.py:475\u001b[0m, in \u001b[0;36m_flatten\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    473\u001b[0m     )\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    476\u001b[0m     k: {\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: v\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: _tobytes(v, k),\n\u001b[1;32m    480\u001b[0m     }\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    482\u001b[0m }\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/safetensors/torch.py:479\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    473\u001b[0m     )\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    476\u001b[0m     k: {\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: v\u001b[38;5;241m.\u001b[39mshape,\n\u001b[0;32m--> 479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    480\u001b[0m     }\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    482\u001b[0m }\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/safetensors/torch.py:404\u001b[0m, in \u001b[0;36m_tobytes\u001b[0;34m(tensor, name)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` which is not allowed. It either means you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms recommended to save\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pack it before saving.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befda412-513b-48e8-a9d4-b40c1d32bef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (k_proj): Linear4bit(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (v_proj): Linear4bit(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (o_proj): Linear4bit(\n",
      "            in_features=4096, out_features=4096, bias=False\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(\n",
      "            in_features=4096, out_features=11008, bias=False\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=16, out_features=11008, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (up_proj): Linear4bit(\n",
      "            in_features=4096, out_features=11008, bias=False\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=16, out_features=11008, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (down_proj): Linear4bit(\n",
      "            in_features=11008, out_features=4096, bias=False\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=11008, out_features=16, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e475b0d-8ae9-4d30-851c-ac230695354e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f269a-2232-4ce4-a807-1902f3074e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Melissa. nobody knows me and i'm 13 years old. I have been writing\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello my name is\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e69697-5567-496e-a183-c672292a3096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 14999\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6187835-23c8-4744-a657-1a6e64ec8c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user', 'answer'],\n",
       "    num_rows: 375\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds =  load_dataset(\"json\", data_files=\"dataset/training_data.json\")\n",
    "ds = ds['train'].train_test_split(test_size=0.3)\n",
    "ds['train']\n",
    "# dataset.train_test_split(test_size=0.3)\n",
    "# dataset\n",
    "# dataset.train_test_split(test_size=0.3)\n",
    "# test_dataset = load_dataset(\"dataset\", split=\"test\")\n",
    "# train_dataset = load_dataset(\"dataset\", split=\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4ee84f5-e794-4a75-838d-ee1abc714416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1,\n",
       "  13866,\n",
       "  338,\n",
       "  385,\n",
       "  15278,\n",
       "  393,\n",
       "  16612,\n",
       "  263,\n",
       "  3414,\n",
       "  29889,\n",
       "  14350,\n",
       "  263,\n",
       "  2933,\n",
       "  393,\n",
       "  7128,\n",
       "  2486,\n",
       "  1614,\n",
       "  2167,\n",
       "  278,\n",
       "  2009,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  2277,\n",
       "  29937,\n",
       "  2799,\n",
       "  4080,\n",
       "  29901,\n",
       "  13,\n",
       "  22110,\n",
       "  21614,\n",
       "  278,\n",
       "  1298,\n",
       "  297,\n",
       "  22556,\n",
       "  29973,\n",
       "  13,\n",
       "  13,\n",
       "  4290,\n",
       "  29901,\n",
       "  13,\n",
       "  29911,\n",
       "  9517,\n",
       "  338,\n",
       "  263,\n",
       "  1153,\n",
       "  3522,\n",
       "  7980,\n",
       "  393,\n",
       "  338,\n",
       "  5318,\n",
       "  2845,\n",
       "  29689,\n",
       "  2750,\n",
       "  263,\n",
       "  2323,\n",
       "  23995,\n",
       "  296,\n",
       "  313,\n",
       "  2976,\n",
       "  793,\n",
       "  29897,\n",
       "  470,\n",
       "  1546,\n",
       "  1023,\n",
       "  10907,\n",
       "  310,\n",
       "  1023,\n",
       "  10769,\n",
       "  1269,\n",
       "  313,\n",
       "  29881,\n",
       "  283,\n",
       "  7586,\n",
       "  467,\n",
       "  7806,\n",
       "  4847,\n",
       "  3913,\n",
       "  263,\n",
       "  22556,\n",
       "  1153,\n",
       "  3522,\n",
       "  393,\n",
       "  338,\n",
       "  851,\n",
       "  686,\n",
       "  411,\n",
       "  13793,\n",
       "  304,\n",
       "  21283,\n",
       "  263,\n",
       "  298,\n",
       "  2952,\n",
       "  14051,\n",
       "  495,\n",
       "  8287,\n",
       "  10664,\n",
       "  411,\n",
       "  7091,\n",
       "  975,\n",
       "  470,\n",
       "  2820,\n",
       "  263,\n",
       "  7787,\n",
       "  322,\n",
       "  964,\n",
       "  278,\n",
       "  23995,\n",
       "  296,\n",
       "  29915,\n",
       "  29879,\n",
       "  8973,\n",
       "  29889,\n",
       "  450,\n",
       "  1203,\n",
       "  310,\n",
       "  278,\n",
       "  3748,\n",
       "  338,\n",
       "  304,\n",
       "  767,\n",
       "  7297,\n",
       "  4090,\n",
       "  276,\n",
       "  278,\n",
       "  8287,\n",
       "  297,\n",
       "  1316,\n",
       "  263,\n",
       "  982,\n",
       "  393,\n",
       "  278,\n",
       "  23995,\n",
       "  296,\n",
       "  338,\n",
       "  451,\n",
       "  2221,\n",
       "  304,\n",
       "  1708,\n",
       "  263,\n",
       "  2854,\n",
       "  736,\n",
       "  29889,\n",
       "  450,\n",
       "  4847,\n",
       "  1058,\n",
       "  338,\n",
       "  9368,\n",
       "  304,\n",
       "  736,\n",
       "  278,\n",
       "  8287,\n",
       "  2854,\n",
       "  368,\n",
       "  674,\n",
       "  451,\n",
       "  11581,\n",
       "  263,\n",
       "  1298,\n",
       "  29892,\n",
       "  1550,\n",
       "  278,\n",
       "  11564,\n",
       "  4847,\n",
       "  674,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  2277,\n",
       "  29937,\n",
       "  13291,\n",
       "  29901,\n",
       "  13,\n",
       "  29909,\n",
       "  4847,\n",
       "  1058,\n",
       "  508,\n",
       "  736,\n",
       "  278,\n",
       "  8287,\n",
       "  2854,\n",
       "  368,\n",
       "  746,\n",
       "  278,\n",
       "  11564,\n",
       "  4847,\n",
       "  9368,\n",
       "  304,\n",
       "  736,\n",
       "  372,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  2277,\n",
       "  29937,\n",
       "  2796],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3824bee-2b94-407f-bd9a-36a45c74f7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Given this paragraph, how many public high schools are in Arlington, Virginia?\n",
      "\n",
      "Input:\n",
      "Arlington Public Schools operates the county's public K-12 education system of 22 elementary schools; 6 middle schools (Dorothy Hamm Middle School, Gunston Middle School, Kenmore Middle School, Swanson Middle School, Thomas Jefferson Middle School, and Williamsburg Middle School); and 3 public high schools (Wakefield High School, Washington-Liberty High School, and Yorktown High School). H-B Woodlawn and Arlington Tech are alternative public schools. Arlington County spends about half of its local revenues on education. For the FY2013 budget, 83 percent of funding was from local revenues, and 12 percent from the state. Per pupil expenditures are expected to average $18,700, well above its neighbors, Fairfax County ($13,600) and Montgomery County ($14,900).\n",
      "\n",
      "### Response:\n",
      "There are 3 public high schools in Arlington, Va.\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(dataset[2]['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7945dfd9-9459-4507-aed8-1e785cc2e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Given this paragraph, how many public high schools are in Arlington, Virginia?\n",
    "\n",
    "Input:\n",
    "Arlington Public Schools operates the county's public K-12 education system of 22 elementary schools; 6 middle schools (Dorothy Hamm Middle School, Gunston Middle School, Kenmore Middle School, Swanson Middle School, Thomas Jefferson Middle School, and Williamsburg Middle School); and 3 public high schools (Wakefield High School, Washington-Liberty High School, and Yorktown High School). H-B Woodlawn and Arlington Tech are alternative public schools. Arlington County spends about half of its local revenues on education. For the FY2013 budget, 83 percent of funding was from local revenues, and 12 percent from the state. Per pupil expenditures are expected to average $18,700, well above its neighbors, Fairfax County ($13,600) and Montgomery County ($14,900).\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8318085b-1fc0-4689-8a76-ab34e6e7f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Given this paragraph, how many public high schools are in Arlington, Virginia?\n",
      "\n",
      "Input:\n",
      "Arlington Public Schools operates the county's public K-12 education system of 22 elementary schools; 6 middle schools (Dorothy Hamm Middle School, Gunston Middle School, Kenmore Middle School, Swanson Middle School, Thomas Jefferson Middle School, and Williamsburg Middle School); and 3 public high schools (Wakefield High School, Washington-Liberty High School, and Yorktown High School). H-B Woodlawn and Arlington Tech are alternative public schools. Arlington County spends about half of its local revenues on education. For the FY2013 budget, 83 percent of funding was from local revenues, and 12 percent from the state. Per pupil expenditures are expected to average $18,700, well above its neighbors, Fairfax County ($13,600) and Montgomery County ($14,900).\n",
      "\n",
      "### Response:\n",
      "There are 3 public high schools in Arlington, Virginia.\n",
      "\n",
      "### End\n",
      "\n",
      "Input:\n",
      "Arlington Public Schools operates the county's public K-12 education system of 22 elementary schools; 6 middle schools (Dorothy Hamm Middle School, Gunston Middle School, Kenmore Middle School, Swanson Middle School, Thomas Jefferson Middle School, and Williamsburg Middle School); and 3 public high schools (Wakefield High\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer(text2, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18586389-aa86-4143-aca0-305b1a54e86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
